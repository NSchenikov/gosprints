Спринт 4: Кеширование и метрики

Задача:

    Кеширование:
        Кэшировать популярные запросы к задачам (например, список новых задач) либо детальные данные по задаче.
        Можно хранить кеш в памяти (например, с помощью sync.Map) или в Redis — если хочется. Redis так же можно запустить докером.
        Настроить механизм протухания (TTL).
    Метрики:
        Собрать ключевые метрики: количество задач, среднее время обработки, число активных WebSocket-соединений.
        Реализовать эндпоинт /metrics для отдачи данных в JSON или формате, удобном для интеграции с Prometheus.

Спринт 5: Расширение функционала – Поиск, метрики и gRPC

Функциональные требования:

    Поиск по задачам:
        Реализовать эндпоинт GET /search?query=... для поиска по полям title и description.
        Настроить полнотекстовый поиск PostgreSQL, создать соответствующие индексы.

    gRPC-сервис:
        Создать базовый gRPC-сервис для операций с задачами (например, методы CreateTask, GetTask, ListTasks).
        Использовать gRPC для внутренней коммуникации между компонентами (например, между API-шлюзом и микросервисом обработки задач).

        __________________________
        К третьему спринту: 
        1) при повторном подключении user к ws что происходит со старым conn? Зависает, создавая утечку памяти или детектор дисконнекта в хэндлере может удалить новое подключение?
        Ответ: Клиент открывает новый WebSocket, NewWSHandler создаёт новый conn. AddClient заменяет старый conn на новый, но не закрывает его. Старый conn продолжает существовать в памяти, а хэндлер всё ещё висит в for { ReadMessage() }. Старая горутина не завершится. RemoveClient срабатывает только когда хэндлер получает ошибку
        upd: имеет ли смысл проверка oldConn != conn? В каких ситуациях это равенство может выполняться? В каких случаях проверка может быть не пройдена? Вопрос именно в том, в какои случае они могут быть равны. 
        Ответ: в моем текущем коде oldConn != conn никогда не будет true. Проверка избыточна. Каждый новый вызов Upgrade создает новое подключение и оно физически не может совпадать со старым. Если для одного и того же conn будет вызываться AddClient то мы не закроем сами себя (без проверки закрыли бы только что активный conn). Если бы не было проверки, то AddClient можно было бы вызывать повторно, но подключение осталось бы прежним (не мой случай). Данная проверка может работать как страховка от self-close

        //добавить в схему queue, worker и notifier

        Проблема: в диспетчере каждые 5 секунд вызывался getByStatus - отсюда и misses.
        Диспетчер использовал тот же репозиторий, что и API. Каждый раз после чистки кэша, трех запросов в статистике получал 24 misses (30 секунд) из-за того, что диспетчер и воркеры создавали большую нагрузку. Разделил taskRepo на apiTaskRepo и workerTaskRepo (кэширующий и не кэширующий). WorkerTaskRepo для использования в воркерах и диспетчере, apiTaskRepo для использования в сервисах и кэшировании. 
        Изменил addClient в hub. Сравнение сложных структур плохо тем, что сравниваются все поля в том числе и мьютекс, который сравниваться не должен - это приводит к неопределенному поведению. Может произойти ложное отрицание или ложное срабатываение. 