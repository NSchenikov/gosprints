Спринт 6: Микросервисная архитектура и Docker

Задача:

    Разделить проект на два (или более) микросервиса:
        Task Service (CRUD, поиск, очереди, часть логики gRPC).
        Notification Service (ответственный за отправку WebSocket-уведомлений и/или email-уведомлений).
    Настроить взаимодействие сервисов через gRPC (или через kafka).
    Настроить работу statefull состояния сервисов (если есть внутренние воркеры)
    Настроить Docker Compose:
        Запуск всех микросервисов.
        База данных (PostgreSQL).
        Redis (если используется).
    При желании добавить мониторинг (Prometheus + Grafana) в Docker Compose, чтобы всё поднималось одной командой.

Цели:

    Освоить подходы к [микро]сервисной архитектуре (разделение ответственности, коммуникация).
    Научиться работать с Docker и Docker Compose для локального/тестового развертывания.
    Понять, как масштабировать систему (увеличивать количество экземпляров «Task Service» при росте нагрузки).

    Предполагаемая архитектура:
    gosprints/

├── task-service/ # существующий проект + адаптация
│ ├── cmd/
│ ├── internal/
│ ├── proto/
│ │ └── events/ # события для Kafka
│ │ └── task_events.proto
│ └── docker/
│
├── notification-service/ # новый сервис
│ ├── cmd/
│ │ └── main.go
│ ├── internal/
│ │ ├── handlers/ # WebSocket хендлеры
│ │ ├── kafka/ # consumer
│ │ ├── notifiers/ # email, ws
│ │ └── models/
│ ├── proto/
│ │ └── events/ # те же proto, что в task-service
│ └── docker/
│
├── api-gateway/ # (можно оставить текущий)
│ ├── cmd/
│ └── internal/
│
├── docker-compose.yml
├── prometheus.yml
└── .env

---

проверить что воркер, изменяя статусы, отражает эти изменения в кэше,т.е. правильно ли работает инвалидация. Написать интеграционный тест на проверку инвалидации кэша.

!) ПО gRPC
создал proto-файлы и нужную архитектуру, подготовил структуру для gRPC-сервера и клиента, переписал handlers/tasks.go под gRPC (сохранил и прямой доступ через сервис)
Проблема: не установился protoc (скореее всего из-за проблем с совместимостью на старой macOS). Еще осталось сделать полнотекстовый поиск

upd:

1. при переносе на новый мак структура моих tasks в перенесенном postgre не сохранилась. Нашел на старом компе нужные папки, заархивировал, перетащил на новую машину, развернул через терминал - оказалось, что postgre по дефолту разворачивает на 5432, а на старом маке я разворачивал на 8000. Гуглил как сменить порт у моих tasks в postgresql, поменял. Установил proto, который не вставал на старой машине из-за старой версии оси, установил плагины
2. накосячил с пакетом pb: в proto был taskv1, а импортировал его как pb. сделал gosprints/internal/grpc/task/pb. Из api/proto/task/v1/task.proto создаются файлы в internal/grpc/task/pb/; он описывает все методы и структуры
3. была проблема в нессответствии типов id в proto и в model (в первом был string, во втором int). В proto сделал int32 и добавил конвертацию int32(id) / int(task.GetId())
4. добавил метод List в репозитории для работы сортировки, фильтра и пагниации
5. пока переписывал хэндлеры под grpc сломал контекст с user_id. Пришлось снова лезть в auth.GetUserFromJWT() и подружить все это с переписанными хэндлерами
6. сломалась структура конструктора NewTaskHandler из-за того что пока переписывал хендлеры убрал уведомления ws. Уведомлениям нужен был taskHandler с hub чтобы работали уведомления
7. сделал полнотекстовый поиск, но в процессе миграции postgresql опять вылезла проблема, связанная с тем что postgre не на своем дефолтном порту. Чтобы это понять пришлось прологировать client, server в gRPC и метод поиска в хэндлере. Еще оказалось что папка с файлом sql для миграции должна лежать в корне, а не в internal
8. Кэш не работает, потому что используется gRPC клиент напрямую, минуя сервисный слой с кэшированием. Я могу в main.go при запуске gRPC заменить baseTaskRepo на apiTaskRepo, но надо будет в taskCacheRepository добавить недостающие методы. А можно просто оставить все как есть, при этом кэш будет работать на уровне HTTP через apiTaskRepo, но gRPC будет ходить напрямую в БД
9. Начал разделять проект на task-service и notification-service. Набросал kafka producer в task-service, перетащил туда старый хэндлер с тасками, убрал оттуда прямые вызовы websocket. В папку notification-service перетащил весь ws из старого internal, накидал kafka consumer, в manager.go сделал шаблон для развития логики уведомлений пользователя по email. Накидал docker-compose, dockerfile для каждого сервиса. Убрал ws hub из структур хэндлеров, добавил в эти структуры eventProducer. Добавил cmd/main.go для каждого из сервисов. В main таск-сервиса создал kafka producer и создал taskHandler с этим kafka продюсером, убрал оттуда старый hub вебсокета. Начал растаскивать старый internal на два сервиса и собираю api-gateway. Путаюсь с импортами в новой архитектуре потому что до конца не понимаю как должен api-gateway взаимодействовать с двумя имеющимися сервисами. Пользователи и задачи у меня сейчас в одной БД и я пока не понимаю как сделать лучше: оставить пользователей в task-service (тогда auth-service будет ходить в task-service по gRPC), создать auth-service (наверное, самый правильный с точки зрения инкапсуляции), или же оставить пользователей в api-gateway (и тогда у gateway будет своя БД для users) ОТВЕТ: Пусть api-gateway обращается за пользователем в taskService
